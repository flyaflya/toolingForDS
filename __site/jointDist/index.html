<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
   <link rel="stylesheet" href="/libs/highlight/github.min.css">
   
  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/poole_hyde.css">
<link rel="stylesheet" href="/css/custom.css">
<!-- style adjustments -->
<style>
  html {font-size: 17px;}
  .franklin-content {position: relative; padding-left: 8%; padding-right: 5%; line-height: 1.35em;}
  @media (min-width: 940px) {
    .franklin-content {width: 100%; margin-left: auto; margin-right: auto;}
  }
  @media (max-width: 768px) {
    .franklin-content {padding-left: 6%; padding-right: 6%;}
  }
</style>
<link rel="icon" href="/assets/favicon.png">

   <title>Joint Distributions</title>  
</head>
<body>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <br>
      <img src="/assets/oxInBarn.jpg" style="width: 120px; height: auto; display: block; margin-left: auto; margin-right: auto">
      <h1><a href="/">Tooling for Data Storytellers</a></h1>
      <p class="lead">Unifying narrative, math, and code.</p>
    </div>
    <nav class="sidebar-nav">
      <a class="sidebar-nav-item " href="/">Home</a>
      <a class="sidebar-nav-item " href="/menu1/">The Vision</a>
      <a class="sidebar-nav-item active" href="/jointDist/">Joint Distributions</a>
      <a class="sidebar-nav-item " href="/menu3/">Metropolis Sampling</a>
      <a class="sidebar-nav-item " href="/menu4/">HTML Page Insert</a>
      <a class="sidebar-nav-item " href="/menu5/">Visualization with Luxor</a>
    </nav>
    <p>&copy; Adam J. Fleischhacker.</p>
  </div>
</div>
<div class="content container">

<!-- Content appended here -->
<div class="franklin-content"><h1 id="joint_distributions"><a href="#joint_distributions" class="header-anchor">Joint Distributions</a></h1>
<p>Given \(n\) random variables \(X_1,\ldots,X_n\), a joint probability distribution, \(\mathcal{P}(X_1,\ldots,X_n)\), assigns a probability value to all possible realizations of a set of random variables.  Using marginalization and/or conditioning, any useful probabilistic query becomes answerable once given a joint distribution; as such, a joint distribution is the gold-standard for representing our uncertainty.</p>
<p>When constructing a joint distribution, however, any size to the problem in terms of numbers of variables and possible realizations, renders a joint distribution:</p>
<blockquote>
<p>&quot;unmanageable from every perspective. Computationally, it is very expensive to manipulate and generally too large to store in memory.  Cognitively, it is impossible to acquire so many numbers from a human expert; moreover, the &#91;probability&#93; numbers are very small and do not correspond to events that people can reasonable contemplate.  Statistically, if we want to learn the distribution from data, we would need ridiculously large amounts of data to estimate the many parameters robustly.&quot; <span class="bibref"><a href="#koller2009">Koller & Friedman (2009)</a></span></p>
</blockquote>
<p>Additionally, the data storyteller cannot motivate their audience with just a joint distribution - the data storyteller needs an <em>explanation</em>;  even a joint distribution with tremendous predictive power might fail to motivate human decision makers.  For the data storyteller, other criteria by which to judge explanations might be equally important.  From our perspective, <em>simplicity</em> - consisting of explanatory values such as <em>concision</em>, <em>parsimony</em>, and <em>elegance</em> <span class="bibref">(<a href="#wojtowicz2021">Wojtowicz & DeDeo (2021)</a>)</span> - is an equally important criteria that audiences use to judge an explanation.  Fortunately, there is a way to get joint distributions and simplicity through pairing factorization of a joint distribution with psuedo-causal constructs.  </p>
<h2 id="the_target_distribution"><a href="#the_target_distribution" class="header-anchor">The Target Distribution</a></h2>
<p>For the moment, let&#39;s assume we have a concise way of representing a joint distribution.  Closely mimicking <span class="bibref"><a href="#betancourt2018">Betancourt (2018)</a></span> in both notation and philosophy, let&#39;s refer to this joint distribution&#39;s density function as \(\pi(q)\) where every point \(q \in \mathcal{Q}\) represents a single realization of \(D\) parameters in sample space \(Q\).  </p>
<p>When we tell stories about density \(\pi\), we will want to do so in a way that summarizes our knowledge.  Mathematically, the summaries we make will all be in the form of expectations of some function \(f\) such the expectation is reduced to an integral over parameter space:</p>
\[
\mathbb{E}[f] = \int_{\mathcal{Q}} dq\, \pi(q)\, f(q).
\]
<p>Integrals, like the above, cannot be evaluated analytically for typical problems of a data storyteller, hence the data storyteller will resort to numerical methods for approximating answers to the above.  The approximations will come in the form of representative samples as output by a class of algorithms known as Monte Carlo methods.  We introduce a few of those algorithms below using the following illustrative example.</p>
<h3 id="an_example_target_distribution"><a href="#an_example_target_distribution" class="header-anchor">An Example Target Distribution</a></h3>
<p>For our example, let&#39;s imagine a game where you will win money based on the output of two random events.  The first random event is that you will get a chance to spin the below spinner.</p>
<img src="/assets/jointDist/output/spinnerWithFriction.gif" alt="">
<p>The number the spinner ends up pointing to, a decimal between 0 and 1, is your <em>win multiplier</em>.  Let&#39;s call it \(X\).  The second random event is that each day the game organizers decide on the <em>max win amount</em>, \(Y\).  Every contestent \(i\) for that day has a chance to win \(W_i\) dollars where \(\$W_i = X_i \times Y\).  Assume you know the probability density function for the spinner:</p>
\[
\pi_X(x) = 6x \times (1-x)    \textrm{  where } x \in (0,1)
\]
<p>We graph the above density function with the following code:</p>
<pre><code class="language-julia">using Gadfly

function π_X&#40;x::Real&#41;  ## type \pi and press &lt;tab&gt; to get π symbol
    if x &gt;&#61; 0 &amp;&amp; x &lt;&#61;1
        6*x*&#40;1-x&#41;
    else
        0
    end
end

# plot&#40;f::Function, lower, upper, elements...; mapping...&#41;
plot&#40;π_X, 0, 1,
     Guide.xlabel&#40;&quot;x&quot;&#41;,
     Guide.ylabel&#40;&quot;π_X&#40;x&#41;&quot;&#41;&#41;</code></pre>

<img src="/assets/jointDist/code\output\densitySpinner.svg" alt="">
<p>We can see that the &quot;higher friction zone&quot; of the spinner leads to more outcomes near 0.5 than outcomes near 0 or 1.</p>
<p>Additional, assume the probability density function for the day&#39;s maximum win amount is also known:</p>
\[
\pi_Y(y) = 2 \times 8 \times \frac{y}{10^5} \times \left(1 - (\frac{y}{10^5})^2\right)^7  \textrm{  where } y \in (0,10^5)      
\]
<p>This is actually derived from a <a href="https://en.wikipedia.org/wiki/Kumaraswamy_distribution">Kumaraswamy distribution</a>, but let&#39;s ignore the obscure distribution name and just plot the density of \(Y\):</p>
<pre><code class="language-julia">function π_Y&#40;y::Real&#41;  ## type \pi and press &lt;tab&gt; to get π symbol
    kumaRealization &#61; y / 100000  ## substitute for simplicity
    if kumaRealization &gt;&#61; 0 &amp;&amp; kumaRealization &lt;&#61;1
        2*8*kumaRealization*&#40;1-kumaRealization^2&#41;^7
    else
        0
    end
end

# plot&#40;f::Function, lower, upper, elements...; mapping...&#41;
using Format
plot&#40;π_Y, 0, 100000,
        Guide.xlabel&#40;&quot;y&quot;&#41;,Guide.ylabel&#40;&quot;π_Y&#40;y&#41;&quot;&#41;,
        Scale.x_continuous&#40;labels &#61; x -&gt; format&#40;x, commas &#61; true&#41;&#41;&#41;</code></pre>

<img src="/assets/jointDist/code\output\densityMaxWin.svg" alt="">
<p>Ultimately, gameplayers would hope to win up to &#36;100,000 &#40;i.e. \(X=1\) and \(Y=10^5\)&#41;, but getting close to this would require one to be very lucky.  So a natural question at this point might be &quot;what winnings can be expected if you get a chance to play this game?&quot;</p>
<p>Let&#39;s answer this by expressing the game in the language of &#40;1&#41;.  Let \(\mathcal{Q}\) be our sample space where each \(q \in \mathcal{Q}\) can be expressed as 2-tuple of \(x \in X\) and \(y \in Y\):</p>
\[
q = (x,y)
\]
<p>where due to the independence of \(X\) and \(Y\), the density function for \(\pi_\mathcal{Q}(q) = \pi_{X,Y}(x,y) = \pi_X(x) \times \pi_Y(y)\):</p>
\[
\pi_{X,Y}(x,y) = 6x \times (1-x) \times 2 \times 8 \times \frac{y}{10^5} \times \left(1 - (\frac{y}{10^5})^2\right)^7\\
\pi_{X,Y}(x,y) = \frac{3xy}{3125} \times (1-x) \times \left(1 - \frac{y^2}{10^{10}}\right)^7
\]
<p>The function, \(f\), that we are interested in is our winnings, \(f(x,y) = x * y\) and our expectation of winnings:</p>
\[
\mathbb{E}[f(x,y)] = \int_{0}^{100000}\int_{0}^{1} dxdy\, \frac{3xy}{3125} \times (1-x) \times \left(1 - \frac{y^2}{10^{10}}\right)^7\, xy\\
\mathbb{E}[f(x,y)] = \int_{0}^{100000}\int_{0}^{1} dxdy\, \frac{3x^2y^2}{3125} \times (1-x) \times \left(1 - \frac{y^2}{10^{10}}\right)^7
\]
<p>I do not know about you, but I see that integral and I do not want to attempt it analytically.  Isn&#39;t there an easier way? </p>
<h3 id="monte_carlo_integration"><a href="#monte_carlo_integration" class="header-anchor">Monte Carlo Integration</a></h3>
<p>One easier way to do integrals is to use Monte Carlo integration.  Recall from calculus &#40;or watch the below video&#41; that an integral, like &#40;6&#41; can be approximated as a summation of small slices of the function being integrated.</p>
<iframe id="integrals" width="100%" height="360"
src="https://www.youtube.com/embed/rfG8ce4nNh0"
frameborder="0"
allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
allowfullscreen></iframe>

<p>Defining \(I = \mathbb{E}[f(x,y)]\), then we can approximate \(I\) by a summation of \(N\) points randomly chosen from sample space \(Q\).  We randomly choose the points to avoid interactions that can occur between an evenly-spaced grid and the integrand that is being estimated &#40;e.g. imagine estimating the value of \(\sin(x)\) by taking these evenly-spaced sample points \(x \in \{0,\pi,2\pi,\ldots\}\)&#41;.  Sampling \((x_1,x_2,\ldots,x_N)\) where each \(x_i \sim Uniform(0,1)\) and \((y_1,y_2,\ldots,y_N)\) where each \(y_i \sim Uniform(0,10^5)\), yields the following approximation for \(I\):</p>
\[
\hat{I} = \frac{1}{N} \sum_{j=1}^{N} \frac{3x_j^2y_j^2}{3125} \times (1-x_j) \times \left(1 - \frac{y_j^2}{10^{10}}\right)^7
\]
<p>In Julia, we can get a grid of points with the below code:</p>
<pre><code class="language-julia">using DataFrames
using Gadfly
using Format

N &#61; 1000  ## sample using 1000 points

gridDF &#61; DataFrame&#40;winningsMultiplier &#61; rand&#40;N&#41;,
                    maxWinnings &#61; 10^5 * rand&#40;N&#41;&#41;

plot&#40;gridDF,
        x &#61; :winningsMultiplier, 
        y &#61; :maxWinnings,
        Scale.y_continuous&#40;labels &#61; x -&gt; format&#40;x, commas &#61; true&#41;&#41;&#41;</code></pre>

<img src="/assets/jointDist/code\output\grid.svg" alt="">
<p>And we can then add information about evaluating the integrand of &#40;6&#41; or equivalently the summand of &#40;7&#41; at each of the 1,000 grid points.</p>
<pre><code class="language-julia">## add summand/integrand values as color and contour
function f&#40;x::Real,y::Real&#41;
    3*x^2*y^2 / 3125 * &#40;1 - x&#41; * &#40;1 - y^2/10^10&#41;^7
end

## add column to DataFrame using above function
gridDF.integrand &#61; f.&#40;gridDF.winningsMultiplier,gridDF.maxWinnings&#41;

plot&#40;gridDF,
        x &#61; :winningsMultiplier, 
        y &#61; :maxWinnings,
        color &#61; :integrand,
        Scale.y_continuous&#40;labels &#61; x -&gt; format&#40;x, commas &#61; true&#41;&#41;&#41;</code></pre>

<img src="/assets/jointDist/code\output\gridColor.svg" alt="">
<p>From the above plot, we see that the largest contributors to \(\hat{I}\) are in a sweetspot of winning multipliers near 60&#37; - 70&#37; and maximum winnings of around &#36;30,000 - &#36;40,000.  These are the points that most contribute to moving the estimate of expected winnings away from zero.  Points above &#36;50,000 in maximum winnings do not contribute as much because they are improbable events.  Additonally, the most probable of all the points, somewhere near 50&#37; and &#36;20,000, is not the largest contributor to the expectation. So it is not the largest points in terms of probability or in terms of total winnings &#40;upper-right hand corner&#41; that contribute the most to our expectation, rather points somewhere in between those extremes  contribute the largest summands.  </p>
<p>In code, our estimate of expected winnings turns out to be:</p>
<pre><code class="language-julia">using Statistics
expectedWinnings &#61; mean&#40;gridDF.integrand&#41;</code></pre>
<pre><code class="plaintext">15173.921953709367</code></pre>
<p>Your calculation for expected winnings may be different than mine &#40;i.e. due to inherent randomness in Monte Carlo integration&#41;.  However, asymptotically if we were to increase \(N\), then our numbers would converge; eventually or at least after infinite samples🤷 &#40;see <a href="https://en.wikipedia.org/wiki/Monte_Carlo_integration">the Wikipedia article on Monte Carlo integration</a>&#41;.  Here is how to see some of my randomly generated summand values - observe that highest maximum winnings does not mean largest contribution to the integrand due to the improbability of those events occurring.</p>
<pre><code class="language-julia">gridDF</code></pre>
<pre><code class="plaintext">1000×3 DataFrame
  Row │ winningsMultiplier  maxWinnings  integrand
      │ Float64             Float64      Float64
──────┼───────────────────────────────────────────────
    1 │          0.484901       78074.8    980.182
    2 │          0.335751       54488.1  18129.3
    3 │          0.816441       31568.0  56138.1
    4 │          0.443356       51583.9  32054.0
    5 │          0.443636       13150.3  16088.8
    6 │          0.123175       36199.0   6260.86
    7 │          0.0944916      88722.1      1.20884
    8 │          0.314851       16508.6  14644.9
  ⋮   │         ⋮                ⋮            ⋮
  994 │          0.928692       78391.9    458.815
  995 │          0.446397       11200.8  12162.7
  996 │          0.958467       32211.9  17653.1
  997 │          0.842134       76305.3   1389.45
  998 │          0.481993       56749.9  24489.5
  999 │          0.143741       91653.9      0.382243
 1000 │          0.885029       22297.2  30078.0
                                      985 rows omitted</code></pre>
<h2 id="the_metropolis_hastings_algorithm"><a href="#the_metropolis_hastings_algorithm" class="header-anchor">The Metropolis Hastings Algorithm</a></h2>
<p>In the above example, we used Monte Carlo integration to calculate an integral that I was too analytically intimidated to attempt.  It gave us useful information, namely that expected winnings are in the neighborhood of &#36;15,000 give or take a few thousand dollars.  </p>
<h3 id="markov_chains"><a href="#markov_chains" class="header-anchor">Markov Chains</a></h3>
<p>A <em>Markov chain</em> is a sequence of points \(q\) in parameter space \(\mathcal{Q}\) generated using a special mapping where each subsequent point \(q'\) is a special stochastic function of its preceding point \(q\).  The special function is known as a <em>Markov transition</em>, specifiying a conditional density function \(\mathbb{T}(q'|q)\) over all potential points to jump to, and will be chosen as to preserve the target distribution:</p>
\[
\pi(q) = \int_Q dq' \, \pi(q') \, \mathbb{T}(q|q')
\]
<p>Constructing a <em>Markov transition</em> that satisfies the above is challenging, but the first large success story of this being done comes from the Metropolis algorithm <span class="bibref">(<a href="#metrop53">Metropolis et al. (1953)</a>)</span> with more general application explained in <span class="bibref"><a href="#hastings1970">Hastings, W. K. (1970)</a></span>.  We review that latter work here.</p>
<h3 id="equation_of_state_calculations"><a href="#equation_of_state_calculations" class="header-anchor">Equation of State Calculations</a></h3>
<p>Imagine \(N\) particles in a square.</p>
<h2 id="references"><a href="#references" class="header-anchor">References</a></h2>
<ul>
<li><p><a id="koller2009" class="anchor"></a> Koller, D., &amp; Friedman, N. &#40;2009&#41;. <em>Probabilistic graphical models: principles and techniques.</em> MIT press.</p>
</li>
<li><p><a id="wojtowicz2021" class="anchor"></a> Wojtowicz, Z., &amp; DeDeo, S. &#40;2021&#41;. <em>From Probability to Consilience: How Explanatory Values Implement Bayesian Reasoning.</em> Trends in Cognitive Sciences <strong>24</strong>&#40;12&#41; pp.981–993.</p>
</li>
<li><p><a id="betancourt2018" class="anchor"></a> Betancourt, M. &#40;2017&#41;. <em>A conceptual introduction to Hamiltonian Monte Carlo.</em> arXiv preprint arXiv:1701.02434.</p>
</li>
<li><p><a id="metrop53" class="anchor"></a> Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., &amp; Teller, E. &#40;1953&#41;. <em>Equation of state calculations by fast computing machines.</em> * The Journal of Chemical Physics, <strong>21</strong>&#40;6&#41;, 1087-1092.</p>
</li>
<li><p><a id="hastings1970" class="anchor"></a> Hastings, W. K. &#40;1970&#41;. <em>Monte Carlo sampling methods using Markov chains and their applications.</em> Biometrika, <strong>57</strong>&#40;1&#41;, 97 - 109.</p>
</li>
</ul>
<h2 id="exercises"><a href="#exercises" class="header-anchor">Exercises</a></h2>
<p>Add the Kumaraswamy Distribution to Distributions.jl.  Compare to Beta Distribution.</p>
<div class="page-foot">
  <div class="copyright">
    &copy; Adam J. Fleischhacker. Last modified: March 26, 2021. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
  </div>
</div>
</div><!-- CONTENT ENDS HERE -->
    </div>  <!-- div: content container -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
        <script src="/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
