<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
   <link rel="stylesheet" href="/libs/highlight/github.min.css">
   
  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/poole_hyde.css">
<link rel="stylesheet" href="/css/custom.css">
<!-- style adjustments -->
<style>
  html {font-size: 17px;}
  .franklin-content {position: relative; padding-left: 8%; padding-right: 5%; line-height: 1.35em;}
  @media (min-width: 940px) {
    .franklin-content {width: 100%; margin-left: auto; margin-right: auto;}
  }
  @media (max-width: 768px) {
    .franklin-content {padding-left: 6%; padding-right: 6%;}
  }
</style>
<link rel="icon" href="/assets/favicon.png">

   <title>Hamiltonian Monte Carlo</title>  
</head>
<body>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <br>
      <img src="/assets/oxInBarn.jpg" style="width: 120px; height: auto; display: block; margin-left: auto; margin-right: auto">
      <h1><a href="/">Tooling for Data Storytellers</a></h1>
      <p class="lead">Unifying narrative, math, and code.</p>
    </div>
    <nav class="sidebar-nav">
      <a class="sidebar-nav-item " href="/">Home</a>
      <a class="sidebar-nav-item " href="/vision/">The Vision</a>
      <a class="sidebar-nav-item " href="/luxorViz/">Visualization with Luxor</a>
      <!-- <a class="sidebar-nav-item {{ispage jointDist/*}}active{{end}}" href="/jointDist/">Joint Distributions</a> -->
      <a class="sidebar-nav-item " href="/mcIntegration/">Monte Carlo Integration</a>
      <a class="sidebar-nav-item " href="/mcmc/">Markov Chain Monte Carlo</a>
      <a class="sidebar-nav-item active" href="/hmc/">Hamiltonian Monte Carlo</a>
    </nav>
    <p>&copy; Adam J. Fleischhacker.</p>
  </div>
</div>
<div class="content container">

<!-- Content appended here -->
<div class="franklin-content"><h1 id="hamiltonian_monte_carlo"><a href="#hamiltonian_monte_carlo" class="header-anchor">Hamiltonian Monte Carlo</a></h1>
<h2 id="important_references_for_this_chapter"><a href="#important_references_for_this_chapter" class="header-anchor">Important References for This Chapter</a></h2>
<ul>
<li><p><span class="bibref"><a href="#betancourt2018">Betancourt (2018)</a></span></p>
</li>
<li><p><span class="bibref"><a href="#neal2011">Neal (2011)</a></span></p>
</li>
</ul>
<h3 id="good_videos"><a href="#good_videos" class="header-anchor">Good Videos</a></h3>
<p><iframe id="hmcDummy" width="100%" height="360"
src="https://www.youtube.com/embed/ZGtezhDaSpM"
frameborder="0"
allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
allowfullscreen></iframe>
 <iframe id="lambertHMC" width="100%" height="360"
src="https://www.youtube.com/embed/a-wydhEuAm0"
frameborder="0"
allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
allowfullscreen></iframe>
 <iframe id="BetancourtHMC" width="100%" height="360"
src="https://www.youtube.com/embed/9ykCU2-W_8Y"
frameborder="0"
allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
allowfullscreen></iframe>
</p>
<h2 id="my_intro_to_hmc"><a href="#my_intro_to_hmc" class="header-anchor">My Intro to HMC</a></h2>
<p>When playing around with random walk Metropolis, we have discovered the importance of the proposal distribution.  Try to jump too far and you end up not accepting proposals - exploration suffers; jump too small and you end up exploring just a fractional part of parameter space - exploration suffers.  In higher dimensions, where parameter spcae moves upwards of say 10 parameters, crafting good proposal distributions is nearly impossible and hence, exploration suffers.  To avoid the potentially slow exploration of state space offered by random-walk proposals, an MCMC method, called <em>Hamiltonian Monte Carlo &#40;HMC&#41;</em>, is shown to be performant when parameter space is comprised solely of continuous random variables.</p>
<p>The physical intuition behind HMC creates a compelling metaphor for how parameter space gets explored.  Think of parameter space as a hilly terrain &#40;pictured below&#41;.  The way you will explore this terrain is by rolling a frictionless ball along the hilly terrain, stopping the ball after a little time, recording its position, slightly moving the ball, and then rolling it again in a new direction.  Some rolls will be fast, and other rolls will be slow; each roll though will maintain constant energy - measured by a function called the <em>Hamiltonian</em> as it explores.  Every time you stop the ball, it should now have been able to wander far awy from its previous point, overcoming a challenge faced by random-walk algorithms.  If we roll intelligently, then after a finite amount of time, the ball will effectively give us an ensemble of points whose distribution is indistinguishable from a sample drawn directly from the target distribution.</p>
<img src="/assets/hmc/code/Whanganui_River.jpg" alt="">
<h3 id="making_the_hilly_terrain"><a href="#making_the_hilly_terrain" class="header-anchor">Making the Hilly Terrain</a></h3>
<p>The terrain of parameter space is not the density function itself, but rather the negative log probability density for a particular point \(q\) in parameter space.  Using the log makes things more numerically stable during computation - taking the <strong>negative</strong> log takes high density areas and makes them the valleys of our terrain attracting the ball in a gravity-like manner.  When the ball explores higher terrain, it will gain potential energy and lose momentum/kinetic energy; when the ball moves to lower terrain, it picks up momentum/kinetic energy and loses momentum.</p>
<p>Let&#39;s explore a space, very similar to our spinner space of the previous chapters.  Let</p>
\[\begin{aligned}
X &\sim \textrm{Beta}(2,2)\\
\frac{Y}{10^5} &\sim \textrm{Beta}(2,8)
\end{aligned}\]
<p>Our parameter space \(Q\) is the product space \(X \times Y\) and each \(q_i = (x_i,y_i)\).  Hence, \(f(q) = f(x,y) = f(x) \times f(y)\).  Our goal in this chapter is to explore this sample space using Hamiltonian Monte Carlo.  To calculate the hilly terrain, we need to calculate the negative log density function:</p>
\[\begin{aligned}
-\log(f(x,y)) &= - (\log(f(x) \times f(y)))\\
                &= - (\log(f(x)) + \log(f(y)))\\
                &= -\log(f(x)) - \log(f(y))
\end{aligned}\]
<p>Let&#39;s represent the negative log density function in code:</p>
<pre><code class="language-julia">using Distributions, Gadfly, Format, DataFrames

## specify random variables
X &#61; Beta&#40;2,2&#41;
unscaledBeta &#61; Beta&#40;2,8&#41;
Y &#61; LocationScale&#40;0, 100000, Beta&#40;2,8&#41;&#41;

## calculate log pdf
function negLogDensFun&#40;x::Real,y::Real&#41;
    -logpdf&#40;X,x&#41; - logpdf&#40;Y,y&#41;  ## logpdf built-in to Distributions
end</code></pre>
<p>We can then use the probability function to graph a grid of points with the negative log density used to color the points.</p>
<pre><code class="language-julia">## sample directly and show on grid
numDraws &#61; 100
plotDF &#61; DataFrame&#40;winningsMultiplier &#61; rand&#40;X,numDraws&#41;,
                    maxWinnings &#61; rand&#40;Y,numDraws&#41;&#41;
plotDF.negLogDens &#61; negLogDensFun.&#40;plotDF.winningsMultiplier,
                                  plotDF.maxWinnings&#41;

# make plot
plot&#40;plotDF, x &#61; :winningsMultiplier,
             y &#61; :maxWinnings, 
             color &#61;:negLogDens,
             Geom.point,
        Coord.cartesian&#40;xmin &#61;0, xmax &#61; 1,
                        ymin &#61; 0, ymax &#61; 10^5&#41;,
        Scale.y_continuous&#40;labels &#61; x -&gt; format&#40;x, commas &#61; true&#41;&#41;&#41;</code></pre>

<img src="/assets/hmc/code\output\negLogPlot.svg" alt="">
<p>Iso-density contours can be added to the plot.   These show a flat area in the middle of the plot with quick decending to the more extreme values.</p>
<pre><code class="language-julia"># make plot
plot&#40;z&#61;&#40;x,y&#41; -&gt; negLogDensFun&#40;x,y&#41;, 
        Geom.contour&#40;levels &#61; &#91;10.5,13,14.5,16,17.5&#93;&#41;,
        Guide.colorkey&#40;title &#61; &quot;NegLogDens&quot;&#41;,
         xmin&#61;&#91;0.001&#93;, xmax&#61;&#91;0.999&#93;, ymin&#61;&#91;0.001&#93;, ymax&#61;&#91;10^4.99&#93;, ## avoid -‚àû by excluding support boundaries to overcome plotting bug  
    Theme&#40;line_width&#61;3pt&#41;,
    layer&#40;plotDF, x &#61; :winningsMultiplier,
    y &#61; :maxWinnings, color &#61;:negLogDens, Geom.point&#41;&#41;</code></pre>

<img src="/assets/hmc/code\output\negLogPlot2.svg" alt="">
<h2 id="exploring_the_hilly_terrain"><a href="#exploring_the_hilly_terrain" class="header-anchor">Exploring the Hilly Terrain</a></h2>
<p>Rather than code up a Hamiltonian Monte Carlo sampler, we will use the <code>DynamicHMC</code> package in Julia.  If you can code up the log-likelihood of a posterior distribtution inside a function of the data and parameters of the model, then you can get HMC output &#40;continuous unobserved parameters only&#41;.</p>
<pre><code class="language-julia">#######################3##
# try DynamicHMC : https://tamaspapp.eu/DynamicHMC.jl/stable/

using TransformVariables, LogDensityProblems, DynamicHMC,
    DynamicHMC.Diagnostics, Parameters, Statistics, Random</code></pre>
<p>The key to success in using this package is to specifiy the log-likelihood function of the density function for the distribution you are trying to sample from.  Note, this package want the log-likelihood and not the negative log-likelihood.</p>
<pre><code class="language-julia">## return loglikelihood over x,y param space
function spinnerProblem&#40;Œ∏&#41;
    @unpack x, y &#61; Œ∏               # extract the parameters
    # log likelihood
    logpdf&#40;Beta&#40;2,2&#41;,x&#41; &#43; 
        logpdf&#40;LocationScale&#40;0, 100000, Beta&#40;2,8&#41;&#41;,y&#41;
end

## test spinner problem with namedTuple argument
spinnerProblem&#40;&#40;x&#61;0.5,y&#61;20000&#41;&#41;</code></pre>
<p>With log-likelihood specified, we now indicate how all parameters can be made unbounded using transofrmation functions.  Samplers like to explore unbounded space - this way they do not need code for boundary conditions.</p>
<pre><code class="language-julia">## transform x and y so sampling is unbounded
spinTrans &#61; as&#40;&#40;x &#61; asùïÄ, y &#61; as&#40;Real,0,100000&#41;&#41;&#41;

## transform the log likelihood so sampling on unbounded space is transformed to sampling on bounded space defined by spinTrans

transformedSpinnerProb &#61; TransformedLogDensity&#40;spinTrans, spinnerProblem&#41;

‚àáspinnerProblem &#61; ADgradient&#40;:ForwardDiff, transformedSpinnerProb&#41;

spinResults &#61; mcmc_with_warmup&#40;Random.GLOBAL_RNG, ‚àáspinnerProblem, 100; reporter &#61; NoProgressReport&#40;&#41;&#41;

## transform back to original parmaeter space
spinPosterior &#61; TransformVariables.transform.&#40;spinTrans, spinResults.chain&#41;
spinPosteriorDF &#61; DataFrame&#40;spinPosterior&#41;  ##works well&#33;</code></pre>
<p>And then, we can add this newly sampled data to our previous plot.</p>
<pre><code class="language-julia"># make plot
spinPosteriorDF.negLogDens &#61; negLogDensFun.&#40;spinPosteriorDF.x,
                                  spinPosteriorDF.y&#41;
plot&#40;z&#61;&#40;x,y&#41; -&gt; negLogDensFun&#40;x,y&#41;, 
        Geom.contour&#40;levels &#61; &#91;10.5,13,14.5,16,17.5&#93;&#41;,
        Guide.colorkey&#40;title &#61; &quot;NegLogDens&quot;&#41;,
         xmin&#61;&#91;0.001&#93;, xmax&#61;&#91;0.999&#93;, ymin&#61;&#91;0.001&#93;, ymax&#61;&#91;10^4.99&#93;, ## avoid -‚àû by excluding support boundaries to overcome plotting bug  
    Theme&#40;line_width&#61;3pt&#41;,
    layer&#40;spinPosteriorDF, x &#61; :x,
    y &#61; :y, color &#61;:negLogDens, Geom.point&#41;&#41;</code></pre>

<img src="/assets/hmc/code\output\negLogPlot3.svg" alt="">
<h2 id="exercise"><a href="#exercise" class="header-anchor">Exercise</a></h2>
<p>This chapter is incomplete, but your final project will involve using HMC to get a posterior distribution.  It will be due May 20th and will have you animating a video that shows some fun and basic 1-dimensional or 2-dimensional HMC example being applied to a simple problem of your choosing.  Following the examples in <span class="bibref"><a href="#neal2011">Neal (2011)</a></span> is highly encouraged.  Use <code>DynamicHMC.jl</code> if it makes your life easier.  I will give hints throughout.  <a href="https://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/">See this blog for inspirational animations</a> - just do not do something identical.  Your final submission will be a paragraph describing the problem you are animating and a <code>.gif</code> animation showing the results.  Be unique and aesthically beautiful.  Specific grading criteria and formal HW instructions will be supplied on May 6.</p>
<h2 id="references"><a href="#references" class="header-anchor">References</a></h2>
<ul>
<li><p><a id="betancourt2018" class="anchor"></a> Betancourt, M. &#40;2017&#41;. <em>A conceptual introduction to Hamiltonian Monte Carlo.</em> arXiv preprint arXiv:1701.02434.</p>
</li>
<li><p><a id="neal2011" class="anchor"></a> Neal, R. M. &#40;2011&#41;. <em>MCMC using Hamiltonian dynamics.</em> Handbook of Markov chain Monte Carlo, 2&#40;11&#41;, 2.</p>
</li>
</ul>
<h2 id="exercise__2"><a href="#exercise__2" class="header-anchor">Exercise</a></h2>
<p>This chapter is incomplete, but your final project will involve using HMC to get a posterior distribution and animating some aspect of the process.  It will be due May 20th and will have you animating a video that shows some fun and basic 1-dimensional or 2-dimensional HMC example being applied to a simple problem of your choosing.  Following the examples in <span class="bibref"><a href="#neal2011">Neal (2011)</a></span> is highly encouraged.  Use <code>DynamicHMC.jl</code> if it makes your life easier.  I will give hints throughout.  <a href="https://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/">See this blog for inspirational animations</a> - just do not do something identical.  Your final submission will be a paragraph describing the problem you are animating and a <code>.gif</code> animation showing the results.  Be unique and aesthically beautiful.  Specific grading criteria and formal HW instructions will be supplied on May 6.</p>
<div class="page-foot">
  <div class="copyright">
    &copy; Adam J. Fleischhacker. Last modified: April 29, 2021. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
  </div>
</div>
</div><!-- CONTENT ENDS HERE -->
    </div>  <!-- div: content container -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
        <script src="/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
