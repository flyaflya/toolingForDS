<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
   <link rel="stylesheet" href="/libs/highlight/github.min.css">
   
  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/poole_hyde.css">
<link rel="stylesheet" href="/css/custom.css">
<!-- style adjustments -->
<style>
  html {font-size: 17px;}
  .franklin-content {position: relative; padding-left: 8%; padding-right: 5%; line-height: 1.35em;}
  @media (min-width: 940px) {
    .franklin-content {width: 100%; margin-left: auto; margin-right: auto;}
  }
  @media (max-width: 768px) {
    .franklin-content {padding-left: 6%; padding-right: 6%;}
  }
</style>
<link rel="icon" href="/assets/favicon.png">

   <title>Joint Distributions</title>  
</head>
<body>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <br>
      <img src="/assets/oxInBarn.jpg" style="width: 120px; height: auto; display: block; margin-left: auto; margin-right: auto">
      <h1><a href="/">Tooling for Data Storytellers</a></h1>
      <p class="lead">Unifying narrative, math, and code.</p>
    </div>
    <nav class="sidebar-nav">
      <a class="sidebar-nav-item " href="/">Home</a>
      <a class="sidebar-nav-item " href="/menu1/">The Vision</a>
      <a class="sidebar-nav-item active" href="/2jointDist/">Joint Distributions</a>
      <a class="sidebar-nav-item " href="/menu3/">Metropolis Sampling</a>
      <a class="sidebar-nav-item " href="/menu4/">HTML Page Insert</a>
      <a class="sidebar-nav-item " href="/menu5/">Visualization with Luxor</a>
    </nav>
    <p>&copy; Adam J. Fleischhacker.</p>
  </div>
</div>
<div class="content container">

<!-- Content appended here -->
<div class="franklin-content"><h1 id="joint_distributions"><a href="#joint_distributions" class="header-anchor">Joint Distributions</a></h1>
<p>Given \(n\) random variables \(X_1,\ldots,X_n\), a joint probability distribution, \(\mathcal{P}(X_1,\ldots,X_n)\), assigns a probability value to all possible realizations of a set of random variables.  Using marginalization and/or conditioning, any useful probabilistic query becomes answerable once given a joint distribution; as such, a joint distribution is the gold-standard for representing our uncertainty.</p>
<p>When constructing a joint distribution, however, any size to the problem in terms of numbers of variables and possible realizations, renders a joint distribution:</p>
<blockquote>
<p>&quot;unmanageable from every perspective. Computationally, it is very expensive to manipulate and generally too large to store in memory.  Cognitively, it is impossible to acquire so many numbers from a human expert; moreover, the &#91;probability&#93; numbers are very small and do not correspond to events that people can reasonable contemplate.  Statistically, if we want to learn the distribution from data, we would need ridiculously large amounts of data to estimate the many parameters robustly.&quot; <span class="bibref"><a href="#koller2009">Koller & Friedman (2009)</a></span></p>
</blockquote>
<p>Additionally, the data storyteller cannot motivate their audience with just a joint distribution - the data storyteller needs an <em>explanation</em>;  even a joint distribution with tremendous predictive power might fail to motivate human decision makers.  For the data storyteller, other criteria by which to judge explanations might be equally important.  From our perspective, <em>simplicity</em> - consisting of explanatory values such as <em>concision</em>, <em>parsimony</em>, and <em>elegance</em> <span class="bibref">(<a href="#wojtowicz2021">Wojtowicz & DeDeo (2021)</a>)</span> - is an equally important criteria that audiences use to judge an explanation.  Fortunately, there is a way to get joint distributions and simplicity through pairing factorization of a joint distribution with psuedo-causal constructs.  </p>
<h2 id="the_target_distribution"><a href="#the_target_distribution" class="header-anchor">The Target Distribution</a></h2>
<p>For the moment, let&#39;s assume we have a concise way of representing a joint distribution.  Closely mimicking <span class="bibref"><a href="#betancourt2018">Betancourt (2018)</a></span> in both notation and philosophy, let&#39;s refer to this joint distribution&#39;s density function as \(\pi(q)\) where every point \(q \in \mathcal{Q}\) represents a single realization of \(D\) parameters in sample space \(Q\).  </p>
<p>When we tell stories about density \(\pi\), we will want to do so in a way that summarizes our knowledge.  Mathematically, the summaries we make will all be in the form of expectations of some function \(f\) such the expectation is reduced to an integral over parameter space:</p>
\[
\mathbb{E}[f] = \int_{\mathcal{Q}} dq\, \pi(q)\, f(q).
\]
<p>Integrals, like the above, cannot be evaluated analytically for typical problems of a data storyteller, hence the data storyteller will resort to numerical methods for approximating answers to the above.  The approximations will come in the form of representative samples.  We introduce a few of those methods here.</p>
<h2 id="the_metropolis_hastings_algorithm"><a href="#the_metropolis_hastings_algorithm" class="header-anchor">The Metropolis Hastings Algorithm</a></h2>
<h3 id="markov_chains"><a href="#markov_chains" class="header-anchor">Markov Chains</a></h3>
<p>A <em>Markov chain</em> is a sequence of points \(q\) in parameter space \(\mathcal{Q}\) generated using a special mapping where each subsequent point \(q'\) is a special stochastic function of its preceding point \(q\).  The special function is known as a <em>Markov transition</em>, specifiying a conditional density function \(\mathbb{T}(q'|q)\) over all potential points to jump to, and will be chosen as to preserve the target distribution:</p>
\[
\pi(q) = \int_Q dq' \, \pi(q') \, \mathbb{T}(q|q')
\]
<p>Constructing a <em>Markov transition</em> that satisfies the above is challenging, but the first large success story of this being done comes from the Metropolis algorithm <span class="bibref">(<a href="#metrop53">Metropolis et al. (1953)</a>)</span> with more general application explained in <span class="bibref"><a href="#hastings1970">Hastings, W. K. (1970)</a></span>.  We review that latter work here.</p>
<h3 id="equation_of_state_calculations"><a href="#equation_of_state_calculations" class="header-anchor">Equation of State Calculations</a></h3>
<p>Imagine \(N\) particles in a square.</p>
<h2 id="references"><a href="#references" class="header-anchor">References</a></h2>
<ul>
<li><p><a id="koller2009" class="anchor"></a> Koller, D., &amp; Friedman, N. &#40;2009&#41;. <em>Probabilistic graphical models: principles and techniques.</em> MIT press.</p>
</li>
<li><p><a id="wojtowicz2021" class="anchor"></a> Wojtowicz, Z., &amp; DeDeo, S. &#40;2021&#41;. <em>From Probability to Consilience: How Explanatory Values Implement Bayesian Reasoning.</em> Trends in Cognitive Sciences <strong>24</strong>&#40;12&#41; pp.981â€“993.</p>
</li>
<li><p><a id="betancourt2018" class="anchor"></a> Betancourt, M. &#40;2017&#41;. <em>A conceptual introduction to Hamiltonian Monte Carlo.</em> arXiv preprint arXiv:1701.02434.</p>
</li>
<li><p><a id="metrop53" class="anchor"></a> Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., &amp; Teller, E. &#40;1953&#41;. <em>Equation of state calculations by fast computing machines.</em> * The Journal of Chemical Physics, <strong>21</strong>&#40;6&#41;, 1087-1092.</p>
</li>
<li><p><a id="hastings1970" class="anchor"></a> Hastings, W. K. &#40;1970&#41;. <em>Monte Carlo sampling methods using Markov chains and their applications.</em> Biometrika, <strong>57</strong>&#40;1&#41;, 97 - 109.</p>
</li>
</ul>
<div class="page-foot">
  <div class="copyright">
    &copy; Adam J. Fleischhacker. Last modified: March 23, 2021. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
  </div>
</div>
</div><!-- CONTENT ENDS HERE -->
    </div>  <!-- div: content container -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
        <script src="/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
